import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
df = pd.read_csv('APP.csv')
df.head()
df.info()
df.describe()
#Check for empty values
missing_values = df.isnull().sum()
missing_values
# Distribute values /in the label column
label_distribution = df['label'].value_counts()
label_distribution
df= df.dropna()
# The distribution of sentiments
df.groupby('label').count().plot(kind='bar')

df['Review_length'] = df['Review'].str.len()

#Text length distribution for each label class
sentiment_text_length = df.groupby('label')['Review_length'].agg(['mean', 'median', 'std'])

#Create a text length distribution chart for each category.
plt.figure(figsize=(10, 6))
for label in df['label'].unique():
  plt.hist(df[df['label'] == label]['Review_length'], bins=20, alpha=0.5, label=label)

plt.xlabel('Review Length')
plt.ylabel('Frequency')
plt.title('Review Length Distribution by Sentiment')
plt.legend()
plt.show()
#import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from gensim.models import Word2Vec

#split

X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['label'], test_size=0.2, random_state=42)

# Convert text categories to numbers
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Convert text to numbers using Tokenizer
max_words = 10000
max_len = 500

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

train_data = pad_sequences(train_sequences, maxlen=max_len)
test_data = pad_sequences(test_sequences, maxlen=max_len)

# Training the Word2Vec model
sentences = [text.split() for text in X_train]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Embedding Matrix
embedding_dim = 100
word_index = tokenizer.word_index

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words and word in w2v_model.wv:
        embedding_matrix[i] = w2v_model.wv[word]

#  Deep Model Building
model = Sequential()
model.add(Embedding(input_dim=max_words,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    input_length=max_len,
                    trainable=False))

model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))
model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

#Preparing the model
optimizer = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

#Model training
history = model.fit(train_data, y_train, epochs=15, batch_size=64, validation_split=0.2, callbacks=[early_stop])
loss, accuracy = model.evaluate(X_test_w2v, y_test_encoded)
print('Test accuracy:', accuracy)

#Predicting results
predictions = model.predict(X_test_w2v)
predicted_labels = (predictions > 0.5).astype(int).flatten()

# F1-score
f1 = f1_score(y_test_encoded, predicted_labels, average='weighted')
print('Test F1 score:', f1)
#  Plot loss

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()
#  Plot acc

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()
test_df["label"] = predicted_labels # Now predicted_labels length matches test_df

#Filter out negative comments only.
df_negative = test_df[test_df["label"] == 0]
#  Save negative comments to a CSV file
df_negative.to_csv("negative_reviews .csv", index=False)
print("✅")
import pandas as pd # Import pandas if it's not already imported
negative_reviews = pd.read_csv("negative_reviews .csv") # Load the CSV into a DataFrame
negative_reviews.head() # Now you can call .head() on it                                                                           # Show the number of negative comments extracted
df_negative.head(), len(df_negative)                                                                                                                                         import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# حذف القيم الفارغة وأخذ عينة لتسريع التنفيذ
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# تحويل النصوص إلى تمثيل رقمي باستخدام TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# تطبيق KMeans لتجميع التعليقات إلى 5 مجموعات
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# إضافة النتائج إلى DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# عرض أول 5 تعليقات في كل مجموعة لفهم الأنماط
for i in range(5):
    print(f"\n🔹 **Cluster {i}**:\n")
    print(sample_reviews[sample_reviews["Cluster"] == i].head(5))# حذف القيم الفارغة وأخذ عينة لتسريع التنفيذ
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# تحويل النصوص إلى تمثيل رقمي باستخدام TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# تطبيق KMeans لتجميع التعليقات إلى 5 مجموعات
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# إضافة النتائج إلى DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# استخراج الكلمات الأكثر شيوعًا في كل Cluster
def get_top_keywords(cluster, X, vectorizer, n_words=5):
    cluster_indices = np.where(clusters == cluster)[0]
    cluster_texts = X[cluster_indices].toarray().sum(axis=0)  # تجميع تردد الكلمات داخل الكلاستر
    top_indices = cluster_texts.argsort()[-n_words:][::-1]  # ترتيب الكلمات تنازليًا
    top_words = [vectorizer.get_feature_names_out()[i] for i in top_indices]
    return top_words

# عرض تحليل لكل Cluster
for i in range(5):
    top_words = get_top_keywords(i, X, vectorizer)
    print(f"\n🔹 **Cluster {i}** (أهم الكلمات: {', '.join(top_words)}):\n")

    # توليد تفسير واضح بناءً على الكلمات المكتشفة
    explanation_templates = [
        f"⚠️ هذا الكلاستر يحتوي على مراجعات تتعلق بمشكلة في {top_words[0]}, {top_words[1]} و {top_words[2]}.",
        f"🚨 تشير البيانات إلى أن المشكلة الرئيسية هنا مرتبطة بـ {top_words[0]} و {top_words[1]}.",
        f"❗ يبدو أن العملاء يشتكون بشكل أساسي من {top_words[0]}, {top_words[1]} و {top_words[2]}.",
    ]
    print(np.random.choice(explanation_templates))  # اختيار تفسير عشوائي لإضافة تنوع

    # عرض 5 أمثلة من التعليقات في هذا الكلاستر
    print("\n📝 **أمثلة على المراجعات في هذا الكلاستر:**")
    print("\n".join(sample_reviews[sample_reviews["Cluster"] == i]["Review"].sample(5, random_state=42).tolist()))
# تحويل النصوص إلى قيم عددية باستخدام CountVectorizer بدلاً من TfidfVectorizer
vectorizer = CountVectorizer(stop_words='english', max_features=5000)  # تقليل عدد الميزات لزيادة الدقة
X = vectorizer.fit_transform(df_negative['Review'])

# تطبيق KMeans لتحديد الكلاسترات
kmeans = KMeans(n_clusters=3, random_state=0)
df_negative['Cluster'] = kmeans.fit_predict(X)

n_topics = 5  # عدد المواضيع التي تريد استخراجها
lda = LDA(n_components=n_topics, random_state=0)

# استخراج المواضيع لكل كلاستر
for cluster_id in range(kmeans.n_clusters):
    cluster_data = df_negative[df_negative['Cluster'] == cluster_id]
    print(f"\nCluster {cluster_id} Summary:")

    if cluster_data.shape[0] == 0:
        print("لا توجد مراجعات كافية لهذا الكلاستر.")
        continue

    # تحويل المراجعات الخاصة بالكلاستر إلى تمثيل عددي
    cluster_reviews = vectorizer.transform(cluster_data['Review'])

    # تدريب LDA على بيانات هذا الكلاستر
    lda.fit(cluster_reviews)

    # استخراج المواضيع
    print("\nالمواضيع الرئيسية لهذا الكلاستر:")
    for topic_idx, topic in enumerate(lda.components_):
        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]
        interpretation = f"الموضوع {topic_idx + 1}: يبدو أن المشكلة الرئيسية تتعلق بـ {', '.join(words[:5])}."
        print(interpretation)

    # عرض بعض المراجعات الخاصة بالكلاستر
    print("\nأمثلة على المراجعات في هذا الكلاستر:")
    print("\n".join(cluster_data['Review'].sample(min(3, len(cluster_data))).tolist()))
