import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
df = pd.read_csv('APP.csv')
df.head()
df.info()
df.describe()
#Check for empty values
missing_values = df.isnull().sum()
missing_values
# Distribute values /in the label column
label_distribution = df['label'].value_counts()
label_distribution
df= df.dropna()
# The distribution of sentiments
df.groupby('label').count().plot(kind='bar')

df['Review_length'] = df['Review'].str.len()

#Text length distribution for each label class
sentiment_text_length = df.groupby('label')['Review_length'].agg(['mean', 'median', 'std'])

#Create a text length distribution chart for each category.
plt.figure(figsize=(10, 6))
for label in df['label'].unique():
  plt.hist(df[df['label'] == label]['Review_length'], bins=20, alpha=0.5, label=label)

plt.xlabel('Review Length')
plt.ylabel('Frequency')
plt.title('Review Length Distribution by Sentiment')
plt.legend()
plt.show()
#import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from gensim.models import Word2Vec

#split

X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['label'], test_size=0.2, random_state=42)

# Convert text categories to numbers
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Convert text to numbers using Tokenizer
max_words = 10000
max_len = 500

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

train_data = pad_sequences(train_sequences, maxlen=max_len)
test_data = pad_sequences(test_sequences, maxlen=max_len)

# Training the Word2Vec model
sentences = [text.split() for text in X_train]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Embedding Matrix
embedding_dim = 100
word_index = tokenizer.word_index

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words and word in w2v_model.wv:
        embedding_matrix[i] = w2v_model.wv[word]

#  Deep Model Building
model = Sequential()
model.add(Embedding(input_dim=max_words,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    input_length=max_len,
                    trainable=False))

model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))
model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

#Preparing the model
optimizer = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

#Model training
history = model.fit(train_data, y_train, epochs=15, batch_size=64, validation_split=0.2, callbacks=[early_stop])
loss, accuracy = model.evaluate(X_test_w2v, y_test_encoded)
print('Test accuracy:', accuracy)

#Predicting results
predictions = model.predict(X_test_w2v)
predicted_labels = (predictions > 0.5).astype(int).flatten()

# F1-score
f1 = f1_score(y_test_encoded, predicted_labels, average='weighted')
print('Test F1 score:', f1)
#  Plot loss

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()
#  Plot acc

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()
test_df["label"] = predicted_labels # Now predicted_labels length matches test_df

#Filter out negative comments only.
df_negative = test_df[test_df["label"] == 0]
#  Save negative comments to a CSV file
df_negative.to_csv("negative_reviews .csv", index=False)
print("âœ…")
import pandas as pd # Import pandas if it's not already imported
negative_reviews = pd.read_csv("negative_reviews .csv") # Load the CSV into a DataFrame
negative_reviews.head() # Now you can call .head() on it                                                                           # Show the number of negative comments extracted
df_negative.head(), len(df_negative)                                                                                                                                         import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Ø­Ø°Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªÙ†ÙÙŠØ°
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø±Ù‚Ù…ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# ØªØ·Ø¨ÙŠÙ‚ KMeans Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¥Ù„Ù‰ 5 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙÙŠ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù„ÙÙ‡Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø·
for i in range(5):
    print(f"\nğŸ”¹ **Cluster {i}**:\n")
    print(sample_reviews[sample_reviews["Cluster"] == i].head(5))# Ø­Ø°Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªÙ†ÙÙŠØ°
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø±Ù‚Ù…ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# ØªØ·Ø¨ÙŠÙ‚ KMeans Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¥Ù„Ù‰ 5 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ù‹Ø§ ÙÙŠ ÙƒÙ„ Cluster
def get_top_keywords(cluster, X, vectorizer, n_words=5):
    cluster_indices = np.where(clusters == cluster)[0]
    cluster_texts = X[cluster_indices].toarray().sum(axis=0)  # ØªØ¬Ù…ÙŠØ¹ ØªØ±Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    top_indices = cluster_texts.argsort()[-n_words:][::-1]  # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙ†Ø§Ø²Ù„ÙŠÙ‹Ø§
    top_words = [vectorizer.get_feature_names_out()[i] for i in top_indices]
    return top_words

# Ø¹Ø±Ø¶ ØªØ­Ù„ÙŠÙ„ Ù„ÙƒÙ„ Cluster
for i in range(5):
    top_words = get_top_keywords(i, X, vectorizer)
    print(f"\nğŸ”¹ **Cluster {i}** (Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {', '.join(top_words)}):\n")

    # ØªÙˆÙ„ÙŠØ¯ ØªÙØ³ÙŠØ± ÙˆØ§Ø¶Ø­ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
    explanation_templates = [
        f"âš ï¸ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ØªØªØ¹Ù„Ù‚ Ø¨Ù…Ø´ÙƒÙ„Ø© ÙÙŠ {top_words[0]}, {top_words[1]} Ùˆ {top_words[2]}.",
        f"ğŸš¨ ØªØ´ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø£Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‡Ù†Ø§ Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ù€ {top_words[0]} Ùˆ {top_words[1]}.",
        f"â— ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙŠØ´ØªÙƒÙˆÙ† Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† {top_words[0]}, {top_words[1]} Ùˆ {top_words[2]}.",
    ]
    print(np.random.choice(explanation_templates))  # Ø§Ø®ØªÙŠØ§Ø± ØªÙØ³ÙŠØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ø¥Ø¶Ø§ÙØ© ØªÙ†ÙˆØ¹

    # Ø¹Ø±Ø¶ 5 Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    print("\nğŸ“ **Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±:**")
    print("\n".join(sample_reviews[sample_reviews["Cluster"] == i]["Review"].sample(5, random_state=42).tolist()))
# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù‚ÙŠÙ… Ø¹Ø¯Ø¯ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… CountVectorizer Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† TfidfVectorizer
vectorizer = CountVectorizer(stop_words='english', max_features=5000)  # ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ù‚Ø©
X = vectorizer.fit_transform(df_negative['Review'])

# ØªØ·Ø¨ÙŠÙ‚ KMeans Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±Ø§Øª
kmeans = KMeans(n_clusters=3, random_state=0)
df_negative['Cluster'] = kmeans.fit_predict(X)

n_topics = 5  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡Ø§
lda = LDA(n_components=n_topics, random_state=0)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù„ÙƒÙ„ ÙƒÙ„Ø§Ø³ØªØ±
for cluster_id in range(kmeans.n_clusters):
    cluster_data = df_negative[df_negative['Cluster'] == cluster_id]
    print(f"\nCluster {cluster_id} Summary:")

    if cluster_data.shape[0] == 0:
        print("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±.")
        continue

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø¹Ø¯Ø¯ÙŠ
    cluster_reviews = vectorizer.transform(cluster_data['Review'])

    # ØªØ¯Ø±ÙŠØ¨ LDA Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    lda.fit(cluster_reviews)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹
    print("\nØ§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±:")
    for topic_idx, topic in enumerate(lda.components_):
        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]
        interpretation = f"Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ {topic_idx + 1}: ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ØªØªØ¹Ù„Ù‚ Ø¨Ù€ {', '.join(words[:5])}."
        print(interpretation)

    # Ø¹Ø±Ø¶ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    print("\nØ£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±:")
    print("\n".join(cluster_data['Review'].sample(min(3, len(cluster_data))).tolist()))
