# -*- coding: utf-8 -*-
"""Copy of APy)(.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DjiPoc-98h7GA0I4ypePE75UIRjK0WWf
"""

"""#Import Necessary Libraries"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from gensim.models import Word2Vec

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from sklearn.preprocessing import LabelEncoder

from tensorflow.keras.optimizers import Adam

from tensorflow.keras.callbacks import EarlyStopping

"""#Load the Dataset"""

df = pd.read_csv('APP.csv')

"""# Explore the Data"""

df.head()

df.info()

df.describe()

#Check for empty values
missing_values = df.isnull().sum()
missing_values

# Distribute values /in the label column
label_distribution = df['label'].value_counts()
label_distribution

df= df.dropna()

# The distribution of sentiments
df.groupby('label').count().plot(kind='bar')

df['Review_length'] = df['Review'].str.len()

#Text length distribution for each label class
sentiment_text_length = df.groupby('label')['Review_length'].agg(['mean', 'median', 'std'])

#Create a text length distribution chart for each category.
plt.figure(figsize=(10, 6))
for label in df['label'].unique():
  plt.hist(df[df['label'] == label]['Review_length'], bins=20, alpha=0.5, label=label)

plt.xlabel('Review Length')
plt.ylabel('Frequency')
plt.title('Review Length Distribution by Sentiment')
plt.legend()
plt.show()

"""# #Preprocessing the Data"""



#import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from gensim.models import Word2Vec

#split

X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['label'], test_size=0.2, random_state=42)

# Convert text categories to numbers
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Convert text to numbers using `Tokenizer`
max_words = 10000
max_len = 500

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

train_data = pad_sequences(train_sequences, maxlen=max_len)
test_data = pad_sequences(test_sequences, maxlen=max_len)

# Training the Word2Vec model
sentences = [text.split() for text in X_train]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Embedding Matrix
embedding_dim = 100
word_index = tokenizer.word_index

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words and word in w2v_model.wv:
        embedding_matrix[i] = w2v_model.wv[word]

#  Deep Model Building
model = Sequential()
model.add(Embedding(input_dim=max_words,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    input_length=max_len,
                    trainable=False))

model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))
model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

#Preparing the model
optimizer = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

#Model training
history = model.fit(train_data, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stop])





"""##split

word2vec

encode

#LSTM
"""

#  Plot loss

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

#  Plot acc

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

loss, accuracy = model.evaluate(train_data, y_train)
print('Test accuracy:', accuracy)

#Predicting results
predictions = model.predict(train_data)
predicted_labels = (predictions > 0.5).astype(int).flatten()

# F1-score
f1 = f1_score( y_train, predicted_labels, average='weighted')
print('Test F1 score:', f1)

"""#NEG"""

# Assuming X_test is a pandas Series
test_df = pd.DataFrame(X_test) # Create a DataFrame from X_test Series

#Predicting results using test_data
predictions = model.predict(test_data)  # Use test_data for prediction
predicted_labels = (predictions > 0.5).astype(int).flatten()

test_df["label"] = predicted_labels # Now predicted_labels length matches test_df

#Filter out negative comments only.
df_negative = test_df[test_df["label"] == 0]
#  Save negative comments to a CSV file
df_negative.to_csv("negative_reviews .csv", index=False)

print("✅")

import pandas as pd # Import pandas if it's not already imported
negative_reviews = pd.read_csv("negative_reviews .csv") # Load the CSV into a DataFrame
negative_reviews.head() # Now you can call .head() on it

# Show the number of negative comments extracted
df_negative.head(), len(df_negative)

"""##Topic Modeling - LDA"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pandas as pd  # Import pandas to read the CSV

# Load the negative reviews from the CSV file
negative_reviews = pd.read_csv("negative_reviews .csv")

# Assuming 'df' contains your reviews, extract a sample
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# تحويل النصوص إلى تمثيل رقمي باستخدام TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# تطبيق LDA لاستخراج 5 مواضيع رئيسية
lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
lda_model.fit(X)

# استخراج الكلمات الأساسية لكل موضوع
words = vectorizer.get_feature_names_out()
topics = {}
for topic_idx, topic in enumerate(lda_model.components_):
    top_words = [words[i] for i in topic.argsort()[:-11:-1]]  # استخراج أفضل 10 كلمات
    topics[f"Topic {topic_idx+1}"] = top_words

topics

"""##Clustering(KMeans)"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# حذف القيم الفارغة وأخذ عينة لتسريع التنفيذ
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# تحويل النصوص إلى تمثيل رقمي باستخدام TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# تطبيق KMeans لتجميع التعليقات إلى 5 مجموعات
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# إضافة النتائج إلى DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# عرض أول 5 تعليقات في كل مجموعة لفهم الأنماط
for i in range(5):
    print(f"\n🔹 **Cluster {i}**:\n")
    print(sample_reviews[sample_reviews["Cluster"] == i].head(5))

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np

# حذف القيم الفارغة وأخذ عينة لتسريع التنفيذ
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# تحويل النصوص إلى تمثيل رقمي باستخدام TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# تطبيق KMeans لتجميع التعليقات إلى 5 مجموعات
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# إضافة النتائج إلى DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# استخراج الكلمات الأكثر شيوعًا في كل Cluster
def get_top_keywords(cluster, X, vectorizer, n_words=5):
    cluster_indices = np.where(clusters == cluster)[0]
    cluster_texts = X[cluster_indices].toarray().sum(axis=0)  # تجميع تردد الكلمات داخل الكلاستر
    top_indices = cluster_texts.argsort()[-n_words:][::-1]  # ترتيب الكلمات تنازليًا
    top_words = [vectorizer.get_feature_names_out()[i] for i in top_indices]
    return top_words

# عرض تحليل لكل Cluster
for i in range(5):
    top_words = get_top_keywords(i, X, vectorizer)
    print(f"\n🔹 **Cluster {i}** (أهم الكلمات: {', '.join(top_words)}):\n")

    # توليد تفسير واضح بناءً على الكلمات المكتشفة
    explanation_templates = [
        f"⚠️ هذا الكلاستر يحتوي على مراجعات تتعلق بمشكلة في {top_words[0]}, {top_words[1]} و {top_words[2]}.",
        f"🚨 تشير البيانات إلى أن المشكلة الرئيسية هنا مرتبطة بـ {top_words[0]} و {top_words[1]}.",
        f"❗ يبدو أن العملاء يشتكون بشكل أساسي من {top_words[0]}, {top_words[1]} و {top_words[2]}.",
    ]
    print(np.random.choice(explanation_templates))  # اختيار تفسير عشوائي لإضافة تنوع

    # عرض 5 أمثلة من التعليقات في هذا الكلاستر
    print("\n📝 **أمثلة على المراجعات في هذا الكلاستر:**")
    print("\n".join(sample_reviews[sample_reviews["Cluster"] == i]["Review"].sample(5, random_state=42).tolist()))

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.cluster import KMeans

# تحويل النصوص إلى قيم عددية باستخدام CountVectorizer بدلاً من TfidfVectorizer
vectorizer = CountVectorizer(stop_words='english', max_features=5000)  # تقليل عدد الميزات لزيادة الدقة
X = vectorizer.fit_transform(df_negative['Review'])

# تطبيق KMeans لتحديد الكلاسترات
kmeans = KMeans(n_clusters=3, random_state=0)
df_negative['Cluster'] = kmeans.fit_predict(X)

n_topics = 5  # عدد المواضيع التي تريد استخراجها
lda = LDA(n_components=n_topics, random_state=0)

# استخراج المواضيع لكل كلاستر
for cluster_id in range(kmeans.n_clusters):
    cluster_data = df_negative[df_negative['Cluster'] == cluster_id]
    print(f"\nCluster {cluster_id} Summary:")

    if cluster_data.shape[0] == 0:
        print("لا توجد مراجعات كافية لهذا الكلاستر.")
        continue

    # تحويل المراجعات الخاصة بالكلاستر إلى تمثيل عددي
    cluster_reviews = vectorizer.transform(cluster_data['Review'])

    # تدريب LDA على بيانات هذا الكلاستر
    lda.fit(cluster_reviews)

    # استخراج المواضيع
    print("\nالمواضيع الرئيسية لهذا الكلاستر:")
    for topic_idx, topic in enumerate(lda.components_):
        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]
        interpretation = f"الموضوع {topic_idx + 1}: يبدو أن المشكلة الرئيسية تتعلق بـ {', '.join(words[:5])}."
        print(interpretation)

    # عرض بعض المراجعات الخاصة بالكلاستر
    print("\nأمثلة على المراجعات في هذا الكلاستر:")
    print("\n".join(cluster_data['Review'].sample(min(3, len(cluster_data))).tolist()))  # طباعة 3 مراجعات عشوائية

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.cluster import KMeans
import random

# تحويل النصوص إلى قيم عددية باستخدام CountVectorizer
vectorizer = CountVectorizer(stop_words='english', max_features=5000)
X = vectorizer.fit_transform(df_negative['Review'])

# تطبيق KMeans لتحديد الكلاسترات
kmeans = KMeans(n_clusters=3, random_state=0)
df_negative['Cluster'] = kmeans.fit_predict(X)

n_topics = 5  # عدد المواضيع التي نريد استخراجها
lda = LDA(n_components=n_topics, random_state=0)

# دالة لتحليل المشكلة وتحويلها إلى جملة تفسيرية واضحة
def generate_problem_statement(words):
    if not words:
        return "❌ لم يتمكن النظام من تحديد المشكلة بوضوح في هذا الكلاستر."

    problem_templates = [
        f"⚠️ العملاء يواجهون مشاكل متكررة تتعلق بـ {words[0]}, {words[1]}, و {words[2]}.",
        f"🚨 المراجعات تشير إلى أن {words[0]} هو السبب الرئيسي للشكوى، إلى جانب {words[1]} و {words[2]}.",
        f"❗ تشير البيانات إلى وجود مشاكل متعلقة بـ {words[0]}, {words[1]}, و {words[2]} والتي تؤثر على رضا العملاء.",
        f"🔴 معظم المراجعات في هذا الكلاستر تذكر مشكلات متكررة في {', '.join(words[:4])}.",
        f"⚡ من الواضح أن المشكلة الأساسية هنا تدور حول {words[0]}, {words[1]} و {words[2]} مما يؤثر سلبًا على التجربة العامة."
    ]

    return random.choice(problem_templates)  # اختيار جملة عشوائية لتعطي تنوعًا في التفسير

# استخراج المواضيع لكل كلاستر
for cluster_id in range(kmeans.n_clusters):
    cluster_data = df_negative[df_negative['Cluster'] == cluster_id]
    print(f"\n📌 **تحليل الكلاستر {cluster_id}:**")

    if cluster_data.shape[0] == 0:
        print("❌ لا توجد مراجعات كافية لهذا الكلاستر.")
        continue

    # تحويل المراجعات الخاصة بالكلاستر إلى تمثيل عددي
    cluster_reviews = vectorizer.transform(cluster_data['Review'])

    # تدريب LDA على بيانات هذا الكلاستر
    lda.fit(cluster_reviews)

    # استخراج المواضيع وتحليلها
    all_words = []
    print("\n🔍 **المواضيع المستخرجة لهذا الكلاستر:**")
    for topic_idx, topic in enumerate(lda.components_):
        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-6:-1]]
        print(f"  🔹 **الموضوع {topic_idx + 1}:** {', '.join(words)}")
        all_words.extend(words)

    # تحديد المشكلة الرئيسية لهذا الكلاستر
    problem_statement = generate_problem_statement(list(set(all_words)))
    print(f"\n🛑 **تحليل المشكلة:** {problem_statement}")

    # عرض بعض المراجعات الخاصة بالكلاستر
    print("\n📝 **أمثلة على المراجعات في هذا الكلاستر:**")
    sample_reviews = cluster_data['Review'].sample(min(3, len(cluster_data))).tolist()
    for i, review in enumerate(sample_reviews, 1):
        print(f"  {i}. {review}")







import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
df = pd.read_csv('APP.csv')
df.head()
df.info()
df.describe()
#Check for empty values
missing_values = df.isnull().sum()
missing_values
# Distribute values /in the label column
label_distribution = df['label'].value_counts()
label_distribution
df= df.dropna()
# The distribution of sentiments
df.groupby('label').count().plot(kind='bar')

df['Review_length'] = df['Review'].str.len()

#Text length distribution for each label class
sentiment_text_length = df.groupby('label')['Review_length'].agg(['mean', 'median', 'std'])

#Create a text length distribution chart for each category.
plt.figure(figsize=(10, 6))
for label in df['label'].unique():
  plt.hist(df[df['label'] == label]['Review_length'], bins=20, alpha=0.5, label=label)

plt.xlabel('Review Length')
plt.ylabel('Frequency')
plt.title('Review Length Distribution by Sentiment')
plt.legend()
plt.show()
#import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from gensim.models import Word2Vec

#split

X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['label'], test_size=0.2, random_state=42)

# Convert text categories to numbers
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Convert text to numbers using `Tokenizer`
max_words = 10000
max_len = 500

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

train_data = pad_sequences(train_sequences, maxlen=max_len)
test_data = pad_sequences(test_sequences, maxlen=max_len)

# Training the Word2Vec model
sentences = [text.split() for text in X_train]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Embedding Matrix
embedding_dim = 100
word_index = tokenizer.word_index

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words and word in w2v_model.wv:
        embedding_matrix[i] = w2v_model.wv[word]

#  Deep Model Building
model = Sequential()
model.add(Embedding(input_dim=max_words,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    input_length=max_len,
                    trainable=False))

model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))
model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

#Preparing the model
optimizer = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

#Model training
history = model.fit(train_data, y_train, epochs=15, batch_size=64, validation_split=0.2, callbacks=[early_stop])
loss, accuracy = model.evaluate(X_test_w2v, y_test_encoded)
print('Test accuracy:', accuracy)

#Predicting results
predictions = model.predict(X_test_w2v)
predicted_labels = (predictions > 0.5).astype(int).flatten()

# F1-score
f1 = f1_score(y_test_encoded, predicted_labels, average='weighted')
print('Test F1 score:', f1)
#  Plot loss

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()
#  Plot acc

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()
# Assuming X_test is a pandas Series
test_df = pd.DataFrame(X_test) # Create a DataFrame from X_test Series
test_df["label"] = predicted_labels

#Filter out negative comments only.
df_negative = test_df[test_df["label"] == 0]
#  Save negative comments to a CSV file
df_negative.to_csv("negative_reviews .csv", index=False)

print("✅")
import pandas as pd # Import pandas if it's not already imported
negative_reviews = pd.read_csv("negative_reviews .csv") # Load the CSV into a DataFrame
negative_reviews.head() # Now you can call .head() on it

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# حذف القيم الفارغة وأخذ عينة لتسريع التنفيذ
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# تحويل النصوص إلى تمثيل رقمي باستخدام TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# تطبيق KMeans لتجميع التعليقات إلى 5 مجموعات
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# إضافة النتائج إلى DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# عرض أول 5 تعليقات في كل مجموعة لفهم الأنماط
for i in range(5):
    print(f"\n🔹 **Cluster {i}**:\n")
    print(sample_reviews[sample_reviews["Cluster"] == i].head(5))
# تحويل النصوص إلى قيم عددية باستخدام TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df_negative['Review'])

# تطبيق KMeans لتحديد الكلاسترات
kmeans = KMeans(n_clusters=3, random_state=0)
#df['Cluster'] = kmeans.fit_predict(X)  #The error was here
df_negative['Cluster'] = kmeans.fit_predict(X) # Assign clusters to df_negative instead of df
n_topics = 5 # عدد المواضيع التي تريد استخراجها
lda = LDA(n_components=n_topics, random_state=0)

# استخراج المواضيع لكل كلاستر
for cluster_id in range(kmeans.n_clusters):
    cluster_data = df_negative[df_negative['Cluster'] == cluster_id] # Use df_negative for cluster_data
    print(f"\nCluster {cluster_id} Summary:")

    # تحويل المراجعات الخاصة بالكلاستر إلى تمثيل عددي
    cluster_reviews = vectorizer.transform(df_negative['Review'])

    # تدريب LDA على بيانات هذا الكلاستر
    lda.fit(cluster_reviews)

    # استخراج المواضيع
    for topic_idx, topic in enumerate(lda.components_):
        print(f"\nTopic #{topic_idx + 1} Interpretation:")

        # ترتيب الكلمات بناءً على الوزن
        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]

        # تحويل الكلمات إلى جملة تفسيرية
        interpretation = "المشكلة الرئيسية في هذا الكلاستر تتعلق بـ " + ", ".join(words[:5]) + "."

        # عرض الجملة التفسيرية
        print(interpretation)

    # عرض المراجعات الخاصة بالكلاستر
    print("\nSample reviews from this cluster:")
    print(df_negative['Review'].head()) # Print negative_reviews instead of cluster_data
