# -*- coding: utf-8 -*-
"""Copy of APy)(.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DjiPoc-98h7GA0I4ypePE75UIRjK0WWf
"""

"""#Import Necessary Libraries"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from gensim.models import Word2Vec

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from sklearn.preprocessing import LabelEncoder

from tensorflow.keras.optimizers import Adam

from tensorflow.keras.callbacks import EarlyStopping

"""#Load the Dataset"""

df = pd.read_csv('APP.csv')

"""# Explore the Data"""

df.head()

df.info()

df.describe()

#Check for empty values
missing_values = df.isnull().sum()
missing_values

# Distribute values /in the label column
label_distribution = df['label'].value_counts()
label_distribution

df= df.dropna()

# The distribution of sentiments
df.groupby('label').count().plot(kind='bar')

df['Review_length'] = df['Review'].str.len()

#Text length distribution for each label class
sentiment_text_length = df.groupby('label')['Review_length'].agg(['mean', 'median', 'std'])

#Create a text length distribution chart for each category.
plt.figure(figsize=(10, 6))
for label in df['label'].unique():
  plt.hist(df[df['label'] == label]['Review_length'], bins=20, alpha=0.5, label=label)

plt.xlabel('Review Length')
plt.ylabel('Frequency')
plt.title('Review Length Distribution by Sentiment')
plt.legend()
plt.show()

"""# #Preprocessing the Data"""



#import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from gensim.models import Word2Vec

#split

X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['label'], test_size=0.2, random_state=42)

# Convert text categories to numbers
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Convert text to numbers using `Tokenizer`
max_words = 10000
max_len = 500

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

train_data = pad_sequences(train_sequences, maxlen=max_len)
test_data = pad_sequences(test_sequences, maxlen=max_len)

# Training the Word2Vec model
sentences = [text.split() for text in X_train]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Embedding Matrix
embedding_dim = 100
word_index = tokenizer.word_index

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words and word in w2v_model.wv:
        embedding_matrix[i] = w2v_model.wv[word]

#  Deep Model Building
model = Sequential()
model.add(Embedding(input_dim=max_words,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    input_length=max_len,
                    trainable=False))

model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))
model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

#Preparing the model
optimizer = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

#Model training
history = model.fit(train_data, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stop])





"""##split

word2vec

encode

#LSTM
"""

#  Plot loss

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

#  Plot acc

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()

loss, accuracy = model.evaluate(train_data, y_train)
print('Test accuracy:', accuracy)

#Predicting results
predictions = model.predict(train_data)
predicted_labels = (predictions > 0.5).astype(int).flatten()

# F1-score
f1 = f1_score( y_train, predicted_labels, average='weighted')
print('Test F1 score:', f1)

"""#NEG"""

# Assuming X_test is a pandas Series
test_df = pd.DataFrame(X_test) # Create a DataFrame from X_test Series

#Predicting results using test_data
predictions = model.predict(test_data)  # Use test_data for prediction
predicted_labels = (predictions > 0.5).astype(int).flatten()

test_df["label"] = predicted_labels # Now predicted_labels length matches test_df

#Filter out negative comments only.
df_negative = test_df[test_df["label"] == 0]
#  Save negative comments to a CSV file
df_negative.to_csv("negative_reviews .csv", index=False)

print("âœ…")

import pandas as pd # Import pandas if it's not already imported
negative_reviews = pd.read_csv("negative_reviews .csv") # Load the CSV into a DataFrame
negative_reviews.head() # Now you can call .head() on it

# Show the number of negative comments extracted
df_negative.head(), len(df_negative)

"""##Topic Modeling - LDA"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pandas as pd  # Import pandas to read the CSV

# Load the negative reviews from the CSV file
negative_reviews = pd.read_csv("negative_reviews .csv")

# Assuming 'df' contains your reviews, extract a sample
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø±Ù‚Ù…ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# ØªØ·Ø¨ÙŠÙ‚ LDA Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ 5 Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø±Ø¦ÙŠØ³ÙŠØ©
lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
lda_model.fit(X)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„ÙƒÙ„ Ù…ÙˆØ¶ÙˆØ¹
words = vectorizer.get_feature_names_out()
topics = {}
for topic_idx, topic in enumerate(lda_model.components_):
    top_words = [words[i] for i in topic.argsort()[:-11:-1]]  # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙØ¶Ù„ 10 ÙƒÙ„Ù…Ø§Øª
    topics[f"Topic {topic_idx+1}"] = top_words

topics

"""##Clustering(KMeans)"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Ø­Ø°Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªÙ†ÙÙŠØ°
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø±Ù‚Ù…ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# ØªØ·Ø¨ÙŠÙ‚ KMeans Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¥Ù„Ù‰ 5 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙÙŠ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù„ÙÙ‡Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø·
for i in range(5):
    print(f"\nğŸ”¹ **Cluster {i}**:\n")
    print(sample_reviews[sample_reviews["Cluster"] == i].head(5))

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np

# Ø­Ø°Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªÙ†ÙÙŠØ°
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø±Ù‚Ù…ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# ØªØ·Ø¨ÙŠÙ‚ KMeans Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¥Ù„Ù‰ 5 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ù‹Ø§ ÙÙŠ ÙƒÙ„ Cluster
def get_top_keywords(cluster, X, vectorizer, n_words=5):
    cluster_indices = np.where(clusters == cluster)[0]
    cluster_texts = X[cluster_indices].toarray().sum(axis=0)  # ØªØ¬Ù…ÙŠØ¹ ØªØ±Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    top_indices = cluster_texts.argsort()[-n_words:][::-1]  # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙ†Ø§Ø²Ù„ÙŠÙ‹Ø§
    top_words = [vectorizer.get_feature_names_out()[i] for i in top_indices]
    return top_words

# Ø¹Ø±Ø¶ ØªØ­Ù„ÙŠÙ„ Ù„ÙƒÙ„ Cluster
for i in range(5):
    top_words = get_top_keywords(i, X, vectorizer)
    print(f"\nğŸ”¹ **Cluster {i}** (Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {', '.join(top_words)}):\n")

    # ØªÙˆÙ„ÙŠØ¯ ØªÙØ³ÙŠØ± ÙˆØ§Ø¶Ø­ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©
    explanation_templates = [
        f"âš ï¸ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ØªØªØ¹Ù„Ù‚ Ø¨Ù…Ø´ÙƒÙ„Ø© ÙÙŠ {top_words[0]}, {top_words[1]} Ùˆ {top_words[2]}.",
        f"ğŸš¨ ØªØ´ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø£Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‡Ù†Ø§ Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ù€ {top_words[0]} Ùˆ {top_words[1]}.",
        f"â— ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙŠØ´ØªÙƒÙˆÙ† Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† {top_words[0]}, {top_words[1]} Ùˆ {top_words[2]}.",
    ]
    print(np.random.choice(explanation_templates))  # Ø§Ø®ØªÙŠØ§Ø± ØªÙØ³ÙŠØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù„Ø¥Ø¶Ø§ÙØ© ØªÙ†ÙˆØ¹

    # Ø¹Ø±Ø¶ 5 Ø£Ù…Ø«Ù„Ø© Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    print("\nğŸ“ **Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±:**")
    print("\n".join(sample_reviews[sample_reviews["Cluster"] == i]["Review"].sample(5, random_state=42).tolist()))

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.cluster import KMeans

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù‚ÙŠÙ… Ø¹Ø¯Ø¯ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… CountVectorizer Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† TfidfVectorizer
vectorizer = CountVectorizer(stop_words='english', max_features=5000)  # ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ù‚Ø©
X = vectorizer.fit_transform(df_negative['Review'])

# ØªØ·Ø¨ÙŠÙ‚ KMeans Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±Ø§Øª
kmeans = KMeans(n_clusters=3, random_state=0)
df_negative['Cluster'] = kmeans.fit_predict(X)

n_topics = 5  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡Ø§
lda = LDA(n_components=n_topics, random_state=0)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù„ÙƒÙ„ ÙƒÙ„Ø§Ø³ØªØ±
for cluster_id in range(kmeans.n_clusters):
    cluster_data = df_negative[df_negative['Cluster'] == cluster_id]
    print(f"\nCluster {cluster_id} Summary:")

    if cluster_data.shape[0] == 0:
        print("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±.")
        continue

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø¹Ø¯Ø¯ÙŠ
    cluster_reviews = vectorizer.transform(cluster_data['Review'])

    # ØªØ¯Ø±ÙŠØ¨ LDA Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    lda.fit(cluster_reviews)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹
    print("\nØ§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±:")
    for topic_idx, topic in enumerate(lda.components_):
        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]
        interpretation = f"Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ {topic_idx + 1}: ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ØªØªØ¹Ù„Ù‚ Ø¨Ù€ {', '.join(words[:5])}."
        print(interpretation)

    # Ø¹Ø±Ø¶ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    print("\nØ£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±:")
    print("\n".join(cluster_data['Review'].sample(min(3, len(cluster_data))).tolist()))  # Ø·Ø¨Ø§Ø¹Ø© 3 Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.cluster import KMeans
import random

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù‚ÙŠÙ… Ø¹Ø¯Ø¯ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… CountVectorizer
vectorizer = CountVectorizer(stop_words='english', max_features=5000)
X = vectorizer.fit_transform(df_negative['Review'])

# ØªØ·Ø¨ÙŠÙ‚ KMeans Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±Ø§Øª
kmeans = KMeans(n_clusters=3, random_state=0)
df_negative['Cluster'] = kmeans.fit_predict(X)

n_topics = 5  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ØªÙŠ Ù†Ø±ÙŠØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡Ø§
lda = LDA(n_components=n_topics, random_state=0)

# Ø¯Ø§Ù„Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ø¬Ù…Ù„Ø© ØªÙØ³ÙŠØ±ÙŠØ© ÙˆØ§Ø¶Ø­Ø©
def generate_problem_statement(words):
    if not words:
        return "âŒ Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨ÙˆØ¶ÙˆØ­ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±."

    problem_templates = [
        f"âš ï¸ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙŠÙˆØ§Ø¬Ù‡ÙˆÙ† Ù…Ø´Ø§ÙƒÙ„ Ù…ØªÙƒØ±Ø±Ø© ØªØªØ¹Ù„Ù‚ Ø¨Ù€ {words[0]}, {words[1]}, Ùˆ {words[2]}.",
        f"ğŸš¨ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù† {words[0]} Ù‡Ùˆ Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø´ÙƒÙˆÙ‰ØŒ Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ {words[1]} Ùˆ {words[2]}.",
        f"â— ØªØ´ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ÙˆØ¬ÙˆØ¯ Ù…Ø´Ø§ÙƒÙ„ Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù€ {words[0]}, {words[1]}, Ùˆ {words[2]} ÙˆØ§Ù„ØªÙŠ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡.",
        f"ğŸ”´ Ù…Ø¹Ø¸Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± ØªØ°ÙƒØ± Ù…Ø´ÙƒÙ„Ø§Øª Ù…ØªÙƒØ±Ø±Ø© ÙÙŠ {', '.join(words[:4])}.",
        f"âš¡ Ù…Ù† Ø§Ù„ÙˆØ§Ø¶Ø­ Ø£Ù† Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù‡Ù†Ø§ ØªØ¯ÙˆØ± Ø­ÙˆÙ„ {words[0]}, {words[1]} Ùˆ {words[2]} Ù…Ù…Ø§ ÙŠØ¤Ø«Ø± Ø³Ù„Ø¨Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¹Ø§Ù…Ø©."
    ]

    return random.choice(problem_templates)  # Ø§Ø®ØªÙŠØ§Ø± Ø¬Ù…Ù„Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„ØªØ¹Ø·ÙŠ ØªÙ†ÙˆØ¹Ù‹Ø§ ÙÙŠ Ø§Ù„ØªÙØ³ÙŠØ±

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù„ÙƒÙ„ ÙƒÙ„Ø§Ø³ØªØ±
for cluster_id in range(kmeans.n_clusters):
    cluster_data = df_negative[df_negative['Cluster'] == cluster_id]
    print(f"\nğŸ“Œ **ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± {cluster_id}:**")

    if cluster_data.shape[0] == 0:
        print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±.")
        continue

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø¹Ø¯Ø¯ÙŠ
    cluster_reviews = vectorizer.transform(cluster_data['Review'])

    # ØªØ¯Ø±ÙŠØ¨ LDA Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    lda.fit(cluster_reviews)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§
    all_words = []
    print("\nğŸ” **Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±:**")
    for topic_idx, topic in enumerate(lda.components_):
        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-6:-1]]
        print(f"  ğŸ”¹ **Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ {topic_idx + 1}:** {', '.join(words)}")
        all_words.extend(words)

    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    problem_statement = generate_problem_statement(list(set(all_words)))
    print(f"\nğŸ›‘ **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:** {problem_statement}")

    # Ø¹Ø±Ø¶ Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    print("\nğŸ“ **Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±:**")
    sample_reviews = cluster_data['Review'].sample(min(3, len(cluster_data))).tolist()
    for i, review in enumerate(sample_reviews, 1):
        print(f"  {i}. {review}")







import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
df = pd.read_csv('APP.csv')
df.head()
df.info()
df.describe()
#Check for empty values
missing_values = df.isnull().sum()
missing_values
# Distribute values /in the label column
label_distribution = df['label'].value_counts()
label_distribution
df= df.dropna()
# The distribution of sentiments
df.groupby('label').count().plot(kind='bar')

df['Review_length'] = df['Review'].str.len()

#Text length distribution for each label class
sentiment_text_length = df.groupby('label')['Review_length'].agg(['mean', 'median', 'std'])

#Create a text length distribution chart for each category.
plt.figure(figsize=(10, 6))
for label in df['label'].unique():
  plt.hist(df[df['label'] == label]['Review_length'], bins=20, alpha=0.5, label=label)

plt.xlabel('Review Length')
plt.ylabel('Frequency')
plt.title('Review Length Distribution by Sentiment')
plt.legend()
plt.show()
#import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from gensim.models import Word2Vec

#split

X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['label'], test_size=0.2, random_state=42)

# Convert text categories to numbers
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Convert text to numbers using `Tokenizer`
max_words = 10000
max_len = 500

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)

train_data = pad_sequences(train_sequences, maxlen=max_len)
test_data = pad_sequences(test_sequences, maxlen=max_len)

# Training the Word2Vec model
sentences = [text.split() for text in X_train]
w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Embedding Matrix
embedding_dim = 100
word_index = tokenizer.word_index

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words and word in w2v_model.wv:
        embedding_matrix[i] = w2v_model.wv[word]

#  Deep Model Building
model = Sequential()
model.add(Embedding(input_dim=max_words,
                    output_dim=embedding_dim,
                    weights=[embedding_matrix],
                    input_length=max_len,
                    trainable=False))

model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))
model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

#Preparing the model
optimizer = Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

#Model training
history = model.fit(train_data, y_train, epochs=15, batch_size=64, validation_split=0.2, callbacks=[early_stop])
loss, accuracy = model.evaluate(X_test_w2v, y_test_encoded)
print('Test accuracy:', accuracy)

#Predicting results
predictions = model.predict(X_test_w2v)
predicted_labels = (predictions > 0.5).astype(int).flatten()

# F1-score
f1 = f1_score(y_test_encoded, predicted_labels, average='weighted')
print('Test F1 score:', f1)
#  Plot loss

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()
#  Plot acc

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.show()
# Assuming X_test is a pandas Series
test_df = pd.DataFrame(X_test) # Create a DataFrame from X_test Series
test_df["label"] = predicted_labels

#Filter out negative comments only.
df_negative = test_df[test_df["label"] == 0]
#  Save negative comments to a CSV file
df_negative.to_csv("negative_reviews .csv", index=False)

print("âœ…")
import pandas as pd # Import pandas if it's not already imported
negative_reviews = pd.read_csv("negative_reviews .csv") # Load the CSV into a DataFrame
negative_reviews.head() # Now you can call .head() on it

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Ø­Ø°Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„ØªÙ†ÙÙŠØ°
sample_reviews = negative_reviews["Review"].dropna().sample(5000, random_state=42)

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø±Ù‚Ù…ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF
vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")
X = vectorizer.fit_transform(sample_reviews)

# ØªØ·Ø¨ÙŠÙ‚ KMeans Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¥Ù„Ù‰ 5 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
clusters = kmeans.fit_predict(X)

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame
sample_reviews = sample_reviews.to_frame()
sample_reviews["Cluster"] = clusters

# Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙÙŠ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù„ÙÙ‡Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø·
for i in range(5):
    print(f"\nğŸ”¹ **Cluster {i}**:\n")
    print(sample_reviews[sample_reviews["Cluster"] == i].head(5))
# ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù‚ÙŠÙ… Ø¹Ø¯Ø¯ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df_negative['Review'])

# ØªØ·Ø¨ÙŠÙ‚ KMeans Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±Ø§Øª
kmeans = KMeans(n_clusters=3, random_state=0)
#df['Cluster'] = kmeans.fit_predict(X)  #The error was here
df_negative['Cluster'] = kmeans.fit_predict(X) # Assign clusters to df_negative instead of df
n_topics = 5 # Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡Ø§
lda = LDA(n_components=n_topics, random_state=0)

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ù„ÙƒÙ„ ÙƒÙ„Ø§Ø³ØªØ±
for cluster_id in range(kmeans.n_clusters):
    cluster_data = df_negative[df_negative['Cluster'] == cluster_id] # Use df_negative for cluster_data
    print(f"\nCluster {cluster_id} Summary:")

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø¹Ø¯Ø¯ÙŠ
    cluster_reviews = vectorizer.transform(df_negative['Review'])

    # ØªØ¯Ø±ÙŠØ¨ LDA Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    lda.fit(cluster_reviews)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹
    for topic_idx, topic in enumerate(lda.components_):
        print(f"\nTopic #{topic_idx + 1} Interpretation:")

        # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ²Ù†
        words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ù‰ Ø¬Ù…Ù„Ø© ØªÙØ³ÙŠØ±ÙŠØ©
        interpretation = "Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ± ØªØªØ¹Ù„Ù‚ Ø¨Ù€ " + ", ".join(words[:5]) + "."

        # Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªÙØ³ÙŠØ±ÙŠØ©
        print(interpretation)

    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„ÙƒÙ„Ø§Ø³ØªØ±
    print("\nSample reviews from this cluster:")
    print(df_negative['Review'].head()) # Print negative_reviews instead of cluster_data
